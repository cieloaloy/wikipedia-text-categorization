{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9609fb55",
   "metadata": {},
   "source": [
    "# **Wikipedia Text Categorization**\n",
    "### Second attempt: Support Vector Machine\n",
    "##### *A machine learning project by Cielo Loy for CSE 151A at UCSD*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e626c99",
   "metadata": {},
   "source": [
    "Preprocessing steps were handled in the first model's creation. I reduced the features of the original dataset from 16 to 3 (name, abstract, infoboxes), and filtered the data into 600000 articles evenly split by 3 categories (arts/entertainment, geography, STEM) using keyword matching.\n",
    "\n",
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab625dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bea66ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('mini-dataset.jsonl', lines=True)\n",
    "df[\"text\"] = df[\"name\"].astype(str) + \" \" + df[\"abstract\"].astype(str)\n",
    "\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(\n",
    "    df[\"text\"], df[\"idx\"], test_size=0.2, random_state=42, stratify=df[\"idx\"]\n",
    ")\n",
    "\n",
    "# SVM model\n",
    "model = sk.pipeline.make_pipeline(\n",
    "    sk.feature_extraction.text.TfidfVectorizer(ngram_range=(1, 2)),\n",
    "    sk.svm.LinearSVC(random_state=42)\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "y_hat = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd94939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.11142048250386383\n",
      "Testing error: 0.18250178461282562\n",
      "\n",
      "The first five ground truths vs prediction of each split:\n",
      "Train:\n",
      "True: 1 Pred: 1\n",
      "True: 2 Pred: 2\n",
      "True: 0 Pred: 0\n",
      "True: 1 Pred: 1\n",
      "True: 1 Pred: 1\n",
      "\n",
      "Test:\n",
      "True: 2 Pred: 2\n",
      "True: 0 Pred: 0\n",
      "True: 0 Pred: 0\n",
      "True: 1 Pred: 0\n",
      "True: 2 Pred: 2\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.827     0.768     0.797     60000\n",
      "           1      0.749     0.804     0.776     50446\n",
      "           2      0.909     0.925     0.917     33843\n",
      "\n",
      "    accuracy                          0.817    144289\n",
      "   macro avg      0.829     0.832     0.830    144289\n",
      "weighted avg      0.819     0.817     0.818    144289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Training error:\", 1 - sk.metrics.accuracy_score(y_train, y_pred))\n",
    "print(\"Testing error:\", 1 - sk.metrics.accuracy_score(y_test, y_hat))\n",
    "\n",
    "print(\"\\nThe first five ground truths vs prediction of each split:\")\n",
    "print(\"Train:\")\n",
    "for i in range(5):\n",
    "    print(\"True:\", y_train.iloc[i], \"Pred:\", y_pred[i])\n",
    "\n",
    "print(\"\\nTest:\")\n",
    "for i in range(5):\n",
    "    print(\"True:\", y_test.iloc[i], \"Pred:\", y_hat[i])\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(sk.metrics.classification_report(y_test, y_hat, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc78579",
   "metadata": {},
   "source": [
    "The training and testing errors are already much better than the Naive Bayes model. There is one misclassification in the ground truth sample, but the testing error of 0.18 is better than 0.24 for the first NB model. Precision and recall are almost universally better by at least a small margin, with the most significant improvements in STEM recall (from 0.723 to 0.935)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b41c15",
   "metadata": {},
   "source": [
    "## Testing different parameters\n",
    "\n",
    "As with the original model, further improvements were shown after some experimentation with the model parameters. Let's once again try some restrictions to unigrams and bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a27ff29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.12545828164309125\n",
      "Testing error: 0.18428293217085157\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.823     0.770     0.796     60000\n",
      "           1      0.753     0.804     0.777     50446\n",
      "           2      0.903     0.914     0.909     33843\n",
      "\n",
      "    accuracy                          0.816    144289\n",
      "   macro avg      0.826     0.829     0.827    144289\n",
      "weighted avg      0.817     0.816     0.816    144289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unigrams only\n",
    "model = sk.pipeline.make_pipeline(\n",
    "    sk.feature_extraction.text.TfidfVectorizer(ngram_range=(1, 1)),\n",
    "    sk.svm.LinearSVC(random_state=42)\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "y_hat = model.predict(X_test)\n",
    "print(\"Training error:\", 1 - sk.metrics.accuracy_score(y_train, y_pred))\n",
    "print(\"Testing error:\", 1 - sk.metrics.accuracy_score(y_test, y_hat))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(sk.metrics.classification_report(y_test, y_hat, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1efb49e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training error: 0.11090415762809358\n",
      "Testing error: 0.18961251377443877\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0      0.815     0.769     0.791     60000\n",
      "           1      0.746     0.793     0.769     50446\n",
      "           2      0.904     0.910     0.907     33843\n",
      "\n",
      "    accuracy                          0.810    144289\n",
      "   macro avg      0.822     0.824     0.822    144289\n",
      "weighted avg      0.812     0.810     0.811    144289\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bigrams only\n",
    "model = sk.pipeline.make_pipeline(\n",
    "    sk.feature_extraction.text.TfidfVectorizer(ngram_range=(2, 2)),\n",
    "    sk.svm.LinearSVC(random_state=42)\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_train)\n",
    "y_hat = model.predict(X_test)\n",
    "print(\"Training error:\", 1 - sk.metrics.accuracy_score(y_train, y_pred))\n",
    "print(\"Testing error:\", 1 - sk.metrics.accuracy_score(y_test, y_hat))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(sk.metrics.classification_report(y_test, y_hat, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39bee83",
   "metadata": {},
   "source": [
    "To my surprise, all three SVM models have very similar metrics regardless of their n-gram parameters. I imagine this is because of NB's feature independence assumption, which is not exactly realistic when applied to language. A linear SVM has the inherent advantage of trying to find the maximum margin no matter what."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
